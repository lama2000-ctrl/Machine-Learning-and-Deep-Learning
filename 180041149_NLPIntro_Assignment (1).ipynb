{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b05ab9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:07.517843Z",
          "iopub.status.busy": "2021-12-07T14:22:07.516685Z",
          "iopub.status.idle": "2021-12-07T14:22:09.389728Z",
          "shell.execute_reply": "2021-12-07T14:22:09.388983Z",
          "shell.execute_reply.started": "2021-12-07T13:17:16.522574Z"
        },
        "papermill": {
          "duration": 1.948899,
          "end_time": "2021-12-07T14:22:09.389908",
          "exception": false,
          "start_time": "2021-12-07T14:22:07.441009",
          "status": "completed"
        },
        "tags": [],
        "id": "05b05ab9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "import re \n",
        "import os \n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f93fc3",
      "metadata": {
        "papermill": {
          "duration": 0.068504,
          "end_time": "2021-12-07T14:22:09.667817",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.599313",
          "status": "completed"
        },
        "tags": [],
        "id": "c9f93fc3"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n",
        "\n",
        "\n",
        "In Natural Language Processing (NLP) the conversion of raw-text to numerical form is called <b>Text Representation</b> and believe me this step is one of the most important steps in the NLP pipeline as if we feed in poor features in ML Model, we will get poor results. In computer science, this is often called “garbage in, garbage out.”\n",
        "\n",
        "<b>I observed in NLP feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a topnotch algorithm to an ordinary text representation.</b>\n",
        "\n",
        "In this notebook, I will discuss various text-representation schemes with their advantages and disadvantages so that you can choose one of the schemes which suit your task most. Our main objective is to transform a given text into numerical form so that it can be fed\n",
        "into NLP and ML algorithms.\n",
        "\n",
        "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0301.png)\n",
        "\n",
        "In this notebook, the focus will be on the dotted box in the figure \n",
        "\n",
        "\n",
        "here write a para on the flow of the notebook later\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88a1cb0",
      "metadata": {
        "papermill": {
          "duration": 0.068209,
          "end_time": "2021-12-07T14:22:09.804899",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.736690",
          "status": "completed"
        },
        "tags": [],
        "id": "c88a1cb0"
      },
      "source": [
        "But before moving on to the Text representation step first we have to get a cleaned dataset which then has to be preprocessed. In this notebook, I will be using only a few basic steps to preprocess the text data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732078de",
      "metadata": {
        "papermill": {
          "duration": 0.069141,
          "end_time": "2021-12-07T14:22:09.943204",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.874063",
          "status": "completed"
        },
        "tags": [],
        "id": "732078de"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Load a Clean Dataset</h1><a id = \"2\" ></a>\n",
        "\n",
        "Kaggle Datasets is one of the best sources to get a clean dataset for this notebook I will be using [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive_.zip"
      ],
      "metadata": {
        "id": "HuBxA1dsVTus",
        "outputId": "094e6953-293c-4f9f-a7cf-d9522f5b865d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HuBxA1dsVTus",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive_.zip\n",
            "  inflating: Tweets.csv              \n",
            "  inflating: database.sqlite         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cca1117",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:10.086920Z",
          "iopub.status.busy": "2021-12-07T14:22:10.086133Z",
          "iopub.status.idle": "2021-12-07T14:22:10.245386Z",
          "shell.execute_reply": "2021-12-07T14:22:10.244778Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.517214Z"
        },
        "papermill": {
          "duration": 0.233149,
          "end_time": "2021-12-07T14:22:10.245531",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.012382",
          "status": "completed"
        },
        "tags": [],
        "id": "1cca1117"
      },
      "outputs": [],
      "source": [
        "clean_data = pd.read_csv('/content/Tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e61d2cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:10.387459Z",
          "iopub.status.busy": "2021-12-07T14:22:10.386765Z",
          "iopub.status.idle": "2021-12-07T14:22:10.413495Z",
          "shell.execute_reply": "2021-12-07T14:22:10.414124Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.663265Z"
        },
        "papermill": {
          "duration": 0.10019,
          "end_time": "2021-12-07T14:22:10.414298",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.314108",
          "status": "completed"
        },
        "tags": [],
        "id": "7e61d2cd",
        "outputId": "0b4b6e9a-143d-4101-94d0-9f287dfab4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
              "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
              "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60cd55bd-f081-4586-a753-d3c03a0f4de5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60cd55bd-f081-4586-a753-d3c03a0f4de5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60cd55bd-f081-4586-a753-d3c03a0f4de5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60cd55bd-f081-4586-a753-d3c03a0f4de5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "clean_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c13b864",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:10.558837Z",
          "iopub.status.busy": "2021-12-07T14:22:10.558114Z",
          "iopub.status.idle": "2021-12-07T14:22:10.596904Z",
          "shell.execute_reply": "2021-12-07T14:22:10.597432Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.694501Z"
        },
        "papermill": {
          "duration": 0.113262,
          "end_time": "2021-12-07T14:22:10.597621",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.484359",
          "status": "completed"
        },
        "tags": [],
        "id": "2c13b864",
        "outputId": "0999309a-87c4-40f4-9a4a-9b461390eb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   tweet_id                      14640 non-null  int64  \n",
            " 1   airline_sentiment             14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            " 3   negativereason                9178 non-null   object \n",
            " 4   negativereason_confidence     10522 non-null  float64\n",
            " 5   airline                       14640 non-null  object \n",
            " 6   airline_sentiment_gold        40 non-null     object \n",
            " 7   name                          14640 non-null  object \n",
            " 8   negativereason_gold           32 non-null     object \n",
            " 9   retweet_count                 14640 non-null  int64  \n",
            " 10  text                          14640 non-null  object \n",
            " 11  tweet_coord                   1019 non-null   object \n",
            " 12  tweet_created                 14640 non-null  object \n",
            " 13  tweet_location                9907 non-null   object \n",
            " 14  user_timezone                 9820 non-null   object \n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n"
          ]
        }
      ],
      "source": [
        "clean_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a42104a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:10.740471Z",
          "iopub.status.busy": "2021-12-07T14:22:10.739435Z",
          "iopub.status.idle": "2021-12-07T14:22:10.972813Z",
          "shell.execute_reply": "2021-12-07T14:22:10.973327Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.735777Z"
        },
        "papermill": {
          "duration": 0.306738,
          "end_time": "2021-12-07T14:22:10.973515",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.666777",
          "status": "completed"
        },
        "tags": [],
        "id": "8a42104a",
        "outputId": "254baa0f-43e3-4356-f10a-7b846bb2feec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='airline_sentiment', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(x = \"airline_sentiment\", data = clean_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849e4b1e",
      "metadata": {
        "papermill": {
          "duration": 0.072146,
          "end_time": "2021-12-07T14:22:11.116201",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.044055",
          "status": "completed"
        },
        "tags": [],
        "id": "849e4b1e"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Basic Text Pre-Processing</h1><a id = \"3\" ></a>\n",
        "\n",
        "Text preprocessing steps include a few essential tasks to further clean the available text data. It includes tasks like:-\n",
        "\n",
        "**1. Stop-Word Removal** : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n",
        "\n",
        "**2. Lower Casing** : Convert all words into the lower case because the upper or lower case may not make a difference for the problem.\n",
        "And we are reducing vocabulary size by doing so. \n",
        "\n",
        "**3. Stemming** : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “walk” and “walking” are both reduced to “walk”).\n",
        "\n",
        "**4. Tokenization** : NLP software typically analyzes text by breaking it up into words (tokens) and sentences.\n",
        "\n",
        "Pre-processing of the text is not the main objective of this notebook that's why I am just covering a few basic steps in a brief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04744cce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:11.266128Z",
          "iopub.status.busy": "2021-12-07T14:22:11.265361Z",
          "iopub.status.idle": "2021-12-07T14:22:11.267682Z",
          "shell.execute_reply": "2021-12-07T14:22:11.268298Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.964893Z"
        },
        "papermill": {
          "duration": 0.080846,
          "end_time": "2021-12-07T14:22:11.268478",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.187632",
          "status": "completed"
        },
        "tags": [],
        "id": "04744cce"
      },
      "outputs": [],
      "source": [
        "# First of all let's drop the columns which we don't required\n",
        "\n",
        "waste_col = ['tweet_id', 'airline_sentiment_confidence',\n",
        "       'negativereason', 'negativereason_confidence', 'airline',\n",
        "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
        "       'retweet_count', 'tweet_coord', 'tweet_created',\n",
        "       'tweet_location', 'user_timezone']\n",
        "\n",
        "data = clean_data.drop(waste_col, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ae080b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:11.418865Z",
          "iopub.status.busy": "2021-12-07T14:22:11.418160Z",
          "iopub.status.idle": "2021-12-07T14:22:11.421007Z",
          "shell.execute_reply": "2021-12-07T14:22:11.421498Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.972057Z"
        },
        "papermill": {
          "duration": 0.082756,
          "end_time": "2021-12-07T14:22:11.421681",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.338925",
          "status": "completed"
        },
        "tags": [],
        "id": "76ae080b",
        "outputId": "7d65c58e-08fc-4e36-c4e6-0dacdece49ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  airline_sentiment                                               text\n",
              "0           neutral                @VirginAmerica What @dhepburn said.\n",
              "1          positive  @VirginAmerica plus you've added commercials t...\n",
              "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
              "3          negative  @VirginAmerica it's really aggressive to blast...\n",
              "4          negative  @VirginAmerica and it's a really big bad thing..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b17d8c48-14d0-44f5-80da-299ea8096ca4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b17d8c48-14d0-44f5-80da-299ea8096ca4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b17d8c48-14d0-44f5-80da-299ea8096ca4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b17d8c48-14d0-44f5-80da-299ea8096ca4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76460d5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:11.569737Z",
          "iopub.status.busy": "2021-12-07T14:22:11.568997Z",
          "iopub.status.idle": "2021-12-07T14:22:11.571009Z",
          "shell.execute_reply": "2021-12-07T14:22:11.571526Z",
          "shell.execute_reply.started": "2021-12-07T13:17:18.992147Z"
        },
        "papermill": {
          "duration": 0.079454,
          "end_time": "2021-12-07T14:22:11.571716",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.492262",
          "status": "completed"
        },
        "tags": [],
        "id": "76460d5b"
      },
      "outputs": [],
      "source": [
        "def sentiment(x):\n",
        "    if x == 'positive':\n",
        "        return 2\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff4f92b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:11.717874Z",
          "iopub.status.busy": "2021-12-07T14:22:11.716842Z",
          "iopub.status.idle": "2021-12-07T14:22:11.852609Z",
          "shell.execute_reply": "2021-12-07T14:22:11.853330Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.000278Z"
        },
        "papermill": {
          "duration": 0.210989,
          "end_time": "2021-12-07T14:22:11.853570",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.642581",
          "status": "completed"
        },
        "tags": [],
        "id": "bff4f92b",
        "outputId": "10db4b70-5617-4e3c-c269-a5a68e5d8c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer \n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "# As this dataset is fetched from twitter so it has lots of people tag in tweets\n",
        "# we will remove them \n",
        "tags = r\"@\\w*\"\n",
        "\n",
        "\n",
        "def preprocess_text(sentence, stem = False):\n",
        "    \n",
        "    sentence = [re.sub(tags, \"\", sentence)]\n",
        "    text = []\n",
        "    for word in sentence:\n",
        "        \n",
        "        if word not in stopwords:\n",
        "            \n",
        "            if stem:\n",
        "                text.append(stemmer.stem(word).lower())\n",
        "            else:\n",
        "                text.append(word.lower())\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a9e697",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.004808Z",
          "iopub.status.busy": "2021-12-07T14:22:12.004036Z",
          "iopub.status.idle": "2021-12-07T14:22:12.008748Z",
          "shell.execute_reply": "2021-12-07T14:22:12.009714Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.103723Z"
        },
        "papermill": {
          "duration": 0.082926,
          "end_time": "2021-12-07T14:22:12.010037",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.927111",
          "status": "completed"
        },
        "tags": [],
        "id": "78a9e697",
        "outputId": "9db6fe0e-8f14-47bf-cb34-56cd5d553c94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal Text ----- @VirginAmerica I &lt;3 pretty graphics. so much better than minimal iconography. :D\n",
            "\n",
            "Preprocessed Text -----  i &lt;3 pretty graphics. so much better than minimal iconography. :d\n"
          ]
        }
      ],
      "source": [
        "print(f\"Orignal Text ----- {data.text[11]}\")\n",
        "print()\n",
        "print(f\"Preprocessed Text ----- {preprocess_text(data.text[11])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805d5248",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.161539Z",
          "iopub.status.busy": "2021-12-07T14:22:12.159924Z",
          "iopub.status.idle": "2021-12-07T14:22:12.371282Z",
          "shell.execute_reply": "2021-12-07T14:22:12.371809Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.112119Z"
        },
        "papermill": {
          "duration": 0.286869,
          "end_time": "2021-12-07T14:22:12.372021",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.085152",
          "status": "completed"
        },
        "tags": [],
        "id": "805d5248",
        "outputId": "23596d2d-034c-42e2-ac97-0575335b7e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  target\n",
              "0                                        what  said.       0\n",
              "1   plus you've added commercials to the experien...       2\n",
              "2   i didn't today... must mean i need to take an...       0\n",
              "3   it's really aggressive to blast obnoxious \"en...       1\n",
              "4           and it's a really big bad thing about it       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f71bcb1-42a7-4d54-9285-f505b3db60d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what  said.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i didn't today... must mean i need to take an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f71bcb1-42a7-4d54-9285-f505b3db60d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f71bcb1-42a7-4d54-9285-f505b3db60d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f71bcb1-42a7-4d54-9285-f505b3db60d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data.text = data.text.map(preprocess_text)\n",
        "data['target'] = data.airline_sentiment.map(sentiment)\n",
        "data = data.drop(['airline_sentiment'], axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d801a4c",
      "metadata": {
        "papermill": {
          "duration": 0.07431,
          "end_time": "2021-12-07T14:22:12.520458",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.446148",
          "status": "completed"
        },
        "tags": [],
        "id": "7d801a4c"
      },
      "source": [
        "Now we have preprocessed textual data so now we can proceed further in this notebook and discuss various text representation approaches in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9edb0b",
      "metadata": {
        "papermill": {
          "duration": 0.072323,
          "end_time": "2021-12-07T14:22:12.670810",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.598487",
          "status": "completed"
        },
        "tags": [],
        "id": "5f9edb0b"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">One-Hot Encoding</h1><a id = \"4\" ></a>\n",
        "\n",
        "\n",
        "In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID (wid) that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s barring the index, where index = wid. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.\n",
        "\n",
        "Consider an Example \n",
        "\n",
        "![](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b46ee1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.818949Z",
          "iopub.status.busy": "2021-12-07T14:22:12.818244Z",
          "iopub.status.idle": "2021-12-07T14:22:12.821795Z",
          "shell.execute_reply": "2021-12-07T14:22:12.822317Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.468086Z"
        },
        "papermill": {
          "duration": 0.079496,
          "end_time": "2021-12-07T14:22:12.822488",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.742992",
          "status": "completed"
        },
        "tags": [],
        "id": "09b46ee1"
      },
      "outputs": [],
      "source": [
        "#this is an example vocabulary just to make concept clear\n",
        "sample_vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'run', 'green', 'tree']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c360d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.970010Z",
          "iopub.status.busy": "2021-12-07T14:22:12.969337Z",
          "iopub.status.idle": "2021-12-07T14:22:17.704052Z",
          "shell.execute_reply": "2021-12-07T14:22:17.703010Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.474194Z"
        },
        "papermill": {
          "duration": 4.809595,
          "end_time": "2021-12-07T14:22:17.704275",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.894680",
          "status": "completed"
        },
        "tags": [],
        "id": "d5c360d5"
      },
      "outputs": [],
      "source": [
        "# vocabulary of words present in dataset\n",
        "data_vocab = []\n",
        "for text in data.text:\n",
        "    for word in text:\n",
        "        if word not in data_vocab:\n",
        "            data_vocab.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6920feb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:17.871363Z",
          "iopub.status.busy": "2021-12-07T14:22:17.870435Z",
          "iopub.status.idle": "2021-12-07T14:22:17.875200Z",
          "shell.execute_reply": "2021-12-07T14:22:17.875690Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.211031Z"
        },
        "papermill": {
          "duration": 0.08584,
          "end_time": "2021-12-07T14:22:17.875860",
          "exception": false,
          "start_time": "2021-12-07T14:22:17.790020",
          "status": "completed"
        },
        "tags": [],
        "id": "e6920feb",
        "outputId": "b53bc497-7367-4d39-88ba-7b3703288aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Hot Representation for sentence \"the cat sat on the mat\" :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#function to return one-hot representation of passed text\n",
        "def get_onehot_representation(text, vocab = data_vocab):\n",
        "    onehot_encoded = []\n",
        "    for word in text:\n",
        "        temp = [0]*len(vocab)\n",
        "        temp[vocab.index(word)-1] = 1\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded\n",
        "\n",
        "print(\"One Hot Representation for sentence \\\"the cat sat on the mat\\\" :\")\n",
        "get_onehot_representation(['the', 'cat', 'sat', 'on', 'the', 'mat'], sample_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c6f31d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:18.027795Z",
          "iopub.status.busy": "2021-12-07T14:22:18.027074Z",
          "iopub.status.idle": "2021-12-07T14:22:18.030188Z",
          "shell.execute_reply": "2021-12-07T14:22:18.030685Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.224048Z"
        },
        "papermill": {
          "duration": 0.082567,
          "end_time": "2021-12-07T14:22:18.030870",
          "exception": false,
          "start_time": "2021-12-07T14:22:17.948303",
          "status": "completed"
        },
        "tags": [],
        "id": "f3c6f31d",
        "outputId": "4938bf88-b371-45ea-a457-825efe7725ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Vocabulary : 209\n",
            "Sample of Vocabulary : []\n"
          ]
        }
      ],
      "source": [
        "print(f'Length of Vocabulary : {len(data_vocab)}')\n",
        "print(f'Sample of Vocabulary : {data_vocab[302 : 312]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a9e8ab0",
      "metadata": {
        "papermill": {
          "duration": 0.07211,
          "end_time": "2021-12-07T14:22:18.175781",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.103671",
          "status": "completed"
        },
        "tags": [],
        "id": "5a9e8ab0"
      },
      "source": [
        "We have 14276 different words in a given dataset thus this implies each word representation for one-hot encoding schema will be of 14276-dimensional vector mark that this much big representation is just for a single word if we consider the representation of a sentence which consist of let say 20 words in it then it will be represented with (20,14276) sized matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae0d3ef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:18.371550Z",
          "iopub.status.busy": "2021-12-07T14:22:18.370111Z",
          "iopub.status.idle": "2021-12-07T14:22:18.375456Z",
          "shell.execute_reply": "2021-12-07T14:22:18.375914Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.240142Z"
        },
        "papermill": {
          "duration": 0.12788,
          "end_time": "2021-12-07T14:22:18.376127",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.248247",
          "status": "completed"
        },
        "tags": [],
        "id": "6ae0d3ef",
        "outputId": "a8121e3b-1207-4e94-848a-3fa0d270fdf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of a single sentence : (94, 209)\n"
          ]
        }
      ],
      "source": [
        "sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)\n",
        "print(f\"Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d00e7b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:18.526924Z",
          "iopub.status.busy": "2021-12-07T14:22:18.525529Z",
          "iopub.status.idle": "2021-12-07T14:22:18.529338Z",
          "shell.execute_reply": "2021-12-07T14:22:18.528715Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.298219Z"
        },
        "papermill": {
          "duration": 0.08036,
          "end_time": "2021-12-07T14:22:18.529486",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.449126",
          "status": "completed"
        },
        "tags": [],
        "id": "9d00e7b1"
      },
      "outputs": [],
      "source": [
        "#one-hot representation for dataset sentences \n",
        "\n",
        "# data.loc[:, 'one_hot_rep'] = data.loc[:, 'text'].map(get_onehot_representation)\n",
        "\n",
        "#if you run this cell it will give you a memory error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0130d6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:18.686726Z",
          "iopub.status.busy": "2021-12-07T14:22:18.685615Z",
          "iopub.status.idle": "2021-12-07T14:22:18.690158Z",
          "shell.execute_reply": "2021-12-07T14:22:18.689593Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.311020Z"
        },
        "papermill": {
          "duration": 0.087987,
          "end_time": "2021-12-07T14:22:18.690306",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.602319",
          "status": "completed"
        },
        "tags": [],
        "id": "ea0130d6",
        "outputId": "5ef7bf3c-ae44-4d3f-ea55-c85710dc5e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  target\n",
              "0                                        what  said.       0\n",
              "1   plus you've added commercials to the experien...       2\n",
              "2   i didn't today... must mean i need to take an...       0\n",
              "3   it's really aggressive to blast obnoxious \"en...       1\n",
              "4           and it's a really big bad thing about it       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5332a045-f5b1-460c-a1cf-04aea50ddd78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what  said.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plus you've added commercials to the experien...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i didn't today... must mean i need to take an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it's really aggressive to blast obnoxious \"en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and it's a really big bad thing about it</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5332a045-f5b1-460c-a1cf-04aea50ddd78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5332a045-f5b1-460c-a1cf-04aea50ddd78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5332a045-f5b1-460c-a1cf-04aea50ddd78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c092a14",
      "metadata": {
        "papermill": {
          "duration": 0.073998,
          "end_time": "2021-12-07T14:22:18.838243",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.764245",
          "status": "completed"
        },
        "tags": [],
        "id": "6c092a14"
      },
      "source": [
        "One-hot encoding is intuitive to understand and straightforward to implement. However, it has lots of disadvantages listed below\n",
        "\n",
        "1. The size of a one-hot vector is directly proportional to the size of the vocabulary and if we consider a real-world vocabulary size it may be in millions so we can not represent a single word with a million-dimensional vector. \n",
        "\n",
        "2. One-hot representation does not give a fixed-length representation for text, i.e., the sentence with 32 words in it and 40 words in it has variable length representation. But for most learning algorithms, we need the feature vectors to be of the same length.\n",
        "\n",
        "3. One-Hot representation gives each word the same weight whether that word is important for the task or not.\n",
        "\n",
        "4. One-Hot representation does not represent the meaning of the word in a proper numerical manner as embedding vectors do. Consider an example word read, reading should have similar real-valued vector representation but in this case, they have different representations. \n",
        "\n",
        "5. Let say we train the model on some article and get the vocabulary of size 10000 but what if we use this vocabulary on that text which contains words that are not present in learned vocabulary. This is Known as **Out Of Vocabulary (OOV)** problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2b43c7",
      "metadata": {
        "papermill": {
          "duration": 0.07376,
          "end_time": "2021-12-07T14:22:18.987246",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.913486",
          "status": "completed"
        },
        "tags": [],
        "id": "3f2b43c7"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of words</h1><a id = \"5\" ></a>\n",
        "\n",
        "Bag of words (BoW) is a classical text representation technique that has been used commonly in NLP, especially in text classification problems. The key idea behind it is as follows: represent the text under consideration as a bag (collection) of words while ignoring the order and context.\n",
        "\n",
        "Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and |V|. Each document in the corpus is then converted into a vector of |V| dimensions were in the ith component of the vector, i = wid, is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.\n",
        "\n",
        "Consider an example:\n",
        "\n",
        "let say we have a vocabulary **V consisting of words --> {the, cat, sat, in, hat, with}** then the bag of word representation of a few sentences will be given as \n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*3IACMnNpwVlCl8kSTJocPA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba511c74",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:19.277629Z",
          "iopub.status.busy": "2021-12-07T14:22:19.269750Z",
          "iopub.status.idle": "2021-12-07T14:22:19.281592Z",
          "shell.execute_reply": "2021-12-07T14:22:19.280946Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.331174Z"
        },
        "papermill": {
          "duration": 0.221195,
          "end_time": "2021-12-07T14:22:19.281750",
          "exception": false,
          "start_time": "2021-12-07T14:22:19.060555",
          "status": "completed"
        },
        "tags": [],
        "id": "ba511c74",
        "outputId": "a9d21593-c15a-4ca4-9dfd-3d17b1a15747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary mapping for given sample corpus : \n",
            " {'the': 4, 'cat': 0, 'sat': 3, 'in': 2, 'hat': 1, 'with': 5}\n",
            "\n",
            "Bag of word Representation of sentence 'the cat cat sat in the hat'\n",
            "[[2 1 1 1 2 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_bow = CountVectorizer()\n",
        "\n",
        "# sample_corpus = [['the', 'cat', 'sat'], \n",
        "#                  ['the', 'cat', 'sat', 'in', 'the', 'hat'],\n",
        "#                  ['the', 'cat', 'with', 'the', 'hat']]\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "sample_bow.fit(sample_corpus)\n",
        "\n",
        "def get_bow_representation(text):\n",
        "        return sample_bow.transform(text)\n",
        "    \n",
        "print(f\"Vocabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\n",
        "print(\"\\nBag of word Representation of sentence 'the cat cat sat in the hat'\")\n",
        "print(get_bow_representation([\"the cat cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8d12d8",
      "metadata": {
        "papermill": {
          "duration": 0.073638,
          "end_time": "2021-12-07T14:22:19.431023",
          "exception": false,
          "start_time": "2021-12-07T14:22:19.357385",
          "status": "completed"
        },
        "tags": [],
        "id": "4e8d12d8"
      },
      "source": [
        "Sometimes, we don’t care about the frequency of occurrence of words in the text and we only want to represent whether a word exists in the text or not. In such cases, we just initialize CountVectorizer with the binary=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d3f35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:19.587910Z",
          "iopub.status.busy": "2021-12-07T14:22:19.586810Z",
          "iopub.status.idle": "2021-12-07T14:22:19.592132Z",
          "shell.execute_reply": "2021-12-07T14:22:19.591593Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.346771Z"
        },
        "papermill": {
          "duration": 0.086886,
          "end_time": "2021-12-07T14:22:19.592276",
          "exception": false,
          "start_time": "2021-12-07T14:22:19.505390",
          "status": "completed"
        },
        "tags": [],
        "id": "e84d3f35",
        "outputId": "d5d0876b-3818-4a76-dfb3-97c011f31121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vacabulary mapping for given sample corpus : \n",
            " {'the': 4, 'cat': 0, 'sat': 3, 'in': 2, 'hat': 1, 'with': 5}\n",
            "\n",
            "Bag of word Representation of sentence 'the the the the cat cat sat in the hat'\n",
            "[[1 1 1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "sample_bow = CountVectorizer(binary = True)\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "sample_bow.fit(sample_corpus)\n",
        "\n",
        "def get_bow_representation(text):\n",
        "        return sample_bow.transform(text)\n",
        "    \n",
        "print(f\"Vacabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\n",
        "print(\"\\nBag of word Representation of sentence 'the the the the cat cat sat in the hat'\")\n",
        "print(get_bow_representation([\"the the the the cat cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1823875",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:19.745364Z",
          "iopub.status.busy": "2021-12-07T14:22:19.744678Z",
          "iopub.status.idle": "2021-12-07T14:22:20.196150Z",
          "shell.execute_reply": "2021-12-07T14:22:20.196698Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.357825Z"
        },
        "papermill": {
          "duration": 0.530116,
          "end_time": "2021-12-07T14:22:20.196874",
          "exception": false,
          "start_time": "2021-12-07T14:22:19.666758",
          "status": "completed"
        },
        "tags": [],
        "id": "f1823875"
      },
      "outputs": [],
      "source": [
        "# generate bag of word representation for given dataset\n",
        "\n",
        "bow = CountVectorizer()\n",
        "bow_rep = bow.fit_transform(data.loc[:, 'text'].astype('str'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed2bd97",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:20.349360Z",
          "iopub.status.busy": "2021-12-07T14:22:20.348686Z",
          "iopub.status.idle": "2021-12-07T14:22:20.351571Z",
          "shell.execute_reply": "2021-12-07T14:22:20.352106Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.836071Z"
        },
        "papermill": {
          "duration": 0.081172,
          "end_time": "2021-12-07T14:22:20.352304",
          "exception": false,
          "start_time": "2021-12-07T14:22:20.271132",
          "status": "completed"
        },
        "tags": [],
        "id": "aed2bd97"
      },
      "outputs": [],
      "source": [
        "# intrested one can see vocabulary of given corpus by uncommenting below code line\n",
        "\n",
        "# bow.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7383a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:20.505496Z",
          "iopub.status.busy": "2021-12-07T14:22:20.504824Z",
          "iopub.status.idle": "2021-12-07T14:22:20.952478Z",
          "shell.execute_reply": "2021-12-07T14:22:20.953221Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.841865Z"
        },
        "papermill": {
          "duration": 0.526904,
          "end_time": "2021-12-07T14:22:20.953452",
          "exception": false,
          "start_time": "2021-12-07T14:22:20.426548",
          "status": "completed"
        },
        "tags": [],
        "id": "dc7383a5",
        "outputId": "0cde9a31-83aa-49ad-cd55-81904eecb5d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Bag of word representaion matrix : (14640, 14238)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of Bag of word representaion matrix : {bow_rep.toarray().shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aac8045",
      "metadata": {
        "papermill": {
          "duration": 0.074945,
          "end_time": "2021-12-07T14:22:21.104849",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.029904",
          "status": "completed"
        },
        "tags": [],
        "id": "2aac8045"
      },
      "source": [
        "As we know One-hot representation does not give a fixed-length representation for text but from the output of the above cell, we can interpret that bag of the word has a fixed-length vector representation (14238 dimensional) for each word but it is too big again in this case also."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e0e573",
      "metadata": {
        "papermill": {
          "duration": 0.073905,
          "end_time": "2021-12-07T14:22:21.253882",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.179977",
          "status": "completed"
        },
        "tags": [],
        "id": "46e0e573"
      },
      "source": [
        "**Advantages of this Bag of words(BoW) encoding** :\n",
        "\n",
        "1. Like one-hot encoding, BoW is fairly simple to understand and implement.\n",
        "\n",
        "2. With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.\n",
        "\n",
        "    Consider an example Where \n",
        "\n",
        "    S1 = \"cat on the mat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
        "    S2 = \"mat on the cat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
        "    S3 = \"dog in the mat\" --> BoW Representation --> {0 1 0 1 1 1 0} <br>\n",
        "\n",
        "    The distance between S1 and S2 is 0 as compared to the distance between S1 and S3, which is 2. Thus, the vector space resulting from the BoW scheme captures the semantic similarity of documents. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
        "\n",
        "3. We have a fixed-length encoding for any sentence of arbitrary length.\n",
        "\n",
        "**Disadvantages of this Bag of words(BoW) encoding** :\n",
        "\n",
        "1. The size of the vector increases with the size of the vocabulary as in our case it is 14238 dimensional. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
        "\n",
        "2. It does not capture the similarity between different words that mean the same thing. Say we have three documents: “walk”, “walked”, and “walking”. BoW vectors of all three documents will be equally apart.\n",
        "\n",
        "3. This representation does not have any way to handle **out of vocabulary (OOV)** words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
        "\n",
        "4. As the name indicates, it is a “bag” of words—word order information is lost in this representation. Both S1 and S2 will have the same representation in this scheme.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be49b0ec",
      "metadata": {
        "papermill": {
          "duration": 0.074554,
          "end_time": "2021-12-07T14:22:21.402773",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.328219",
          "status": "completed"
        },
        "tags": [],
        "id": "be49b0ec"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of N-Grams</h1><a id = \"6\" ></a>\n",
        "\n",
        "All the representation schemes we’ve seen so far treat words as independent units. There is no notion of phrases or word order. The bag-of-n-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram.\n",
        "\n",
        "**One can simply say Bag of words (BoW) is a special case of the Bag of n-grams having n = 1.**\n",
        "\n",
        "The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.\n",
        "\n",
        "Consider an Example: \n",
        "\n",
        "![](https://i.stack.imgur.com/8ARA1.png)\n",
        "\n",
        "\n",
        "The following code cell shows an example of a BoN representation considering 1–3 n-gram word features to represent the corpus that we’ve used so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac2dbec",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:21.559902Z",
          "iopub.status.busy": "2021-12-07T14:22:21.559223Z",
          "iopub.status.idle": "2021-12-07T14:22:21.564233Z",
          "shell.execute_reply": "2021-12-07T14:22:21.564824Z",
          "shell.execute_reply.started": "2021-12-07T13:17:25.300886Z"
        },
        "papermill": {
          "duration": 0.087884,
          "end_time": "2021-12-07T14:22:21.565015",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.477131",
          "status": "completed"
        },
        "tags": [],
        "id": "dac2dbec",
        "outputId": "38586a67-a230-4fec-d39e-846784c7aca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Vocabulary mapping for given sample corpus : \n",
            " {'the': 4, 'cat': 0, 'sat': 3, 'in': 2, 'hat': 1, 'with': 5}\n",
            "\n",
            "Bag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'\n",
            "[[2 1 1 1 2 0]]\n"
          ]
        }
      ],
      "source": [
        "# Bag of 1-gram (unigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_boN = CountVectorizer(ngram_range = (1, 1))\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "sample_boN.fit(sample_corpus)\n",
        "\n",
        "def get_boN_representation(text):\n",
        "        return sample_boN.transform(text)\n",
        "    \n",
        "print(f\"Unigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
        "print(\"\\nBag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
        "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72339ae4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:21.718436Z",
          "iopub.status.busy": "2021-12-07T14:22:21.717771Z",
          "iopub.status.idle": "2021-12-07T14:22:21.725744Z",
          "shell.execute_reply": "2021-12-07T14:22:21.726317Z",
          "shell.execute_reply.started": "2021-12-07T13:17:25.311601Z"
        },
        "papermill": {
          "duration": 0.086755,
          "end_time": "2021-12-07T14:22:21.726496",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.639741",
          "status": "completed"
        },
        "tags": [],
        "id": "72339ae4",
        "outputId": "1abd32c6-fcfb-407f-db48-7219e2b4c952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Vocabulary mapping for given sample corpus : \n",
            " {'the cat': 4, 'cat sat': 0, 'sat in': 3, 'in the': 2, 'the hat': 5, 'cat with': 1, 'with the': 6}\n",
            "\n",
            "Bag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'\n",
            "[[1 0 1 1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# Bag of 2-gram (bigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_boN = CountVectorizer(ngram_range = (2, 2))\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "sample_boN.fit(sample_corpus)\n",
        "\n",
        "def get_boN_representation(text):\n",
        "        return sample_boN.transform(text)\n",
        "    \n",
        "print(f\"Bigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
        "print(\"\\nBag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
        "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d0129e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:21.885259Z",
          "iopub.status.busy": "2021-12-07T14:22:21.884061Z",
          "iopub.status.idle": "2021-12-07T14:22:21.889664Z",
          "shell.execute_reply": "2021-12-07T14:22:21.890402Z",
          "shell.execute_reply.started": "2021-12-07T13:17:25.325155Z"
        },
        "papermill": {
          "duration": 0.088166,
          "end_time": "2021-12-07T14:22:21.890627",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.802461",
          "status": "completed"
        },
        "tags": [],
        "id": "c0d0129e",
        "outputId": "4d943133-93cb-48f0-87ca-e472851542ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram Vocabulary mapping for given sample corpus : \n",
            " {'the cat sat': 4, 'cat sat in': 0, 'sat in the': 3, 'in the hat': 2, 'the cat with': 5, 'cat with the': 1, 'with the hat': 6}\n",
            "\n",
            "Bag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'\n",
            "[[1 0 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Bag of 3-gram (trigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_boN = CountVectorizer(ngram_range = (3, 3))\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "sample_boN.fit(sample_corpus)\n",
        "\n",
        "def get_boN_representation(text):\n",
        "        return sample_boN.transform(text)\n",
        "    \n",
        "print(f\"Trigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\n",
        "print(\"\\nBag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'\")\n",
        "print(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cafa10d",
      "metadata": {
        "papermill": {
          "duration": 0.075393,
          "end_time": "2021-12-07T14:22:22.042863",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.967470",
          "status": "completed"
        },
        "tags": [],
        "id": "6cafa10d"
      },
      "source": [
        "**Here are the main advantages and disadvantages of BoN Representation:**\n",
        "\n",
        "1. It captures some context and word-order information in the form of n-grams.\n",
        "\n",
        "2. Thus, the resulting vector space can capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n",
        "\n",
        "3. As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
        "\n",
        "4. It still provides no way to address the **out of vocabulary(OOV)** problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2d4709",
      "metadata": {
        "papermill": {
          "duration": 0.07612,
          "end_time": "2021-12-07T14:22:22.195176",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.119056",
          "status": "completed"
        },
        "tags": [],
        "id": "2c2d4709"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">TF-IDF</h1><a id = \"7\" ></a>\n",
        "\n",
        "\n",
        "In all the three approaches we’ve seen so far, all the words in the text are treated as equally important—there’s no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n",
        "\n",
        "The intuition behind TF-IDF is as follows: if a word w appears many times in a sentence S1 but does not occur much in the rest of the Sentences Sn in the corpus, then the word w must be of great importance to the Sentence S1. The importance of w should increase in proportion to its frequency in S1 (how many times that word occurs in sentence S1), but at the same time, its importance should decrease in proportion to the word’s frequency in other Sentence Sn in the corpus. **Mathematically, this is captured using two quantities: TF and IDF. The two are then multiplied to arrive at the TF-IDF score.**\n",
        "\n",
        "**TF (term frequency) measures how often a term or word occurs in a given document.**\n",
        "\n",
        "Mathematical Expression of TF\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
        "\n",
        "**IDF (inverse document frequency)** measures the importance of the term across a corpus. In computing TF, all terms are given equal importance (weightage). However, it’s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term t is calculated as follows:\n",
        "\n",
        "![](https://mungingdata.files.wordpress.com/2017/11/tfidf.png)\n",
        "\n",
        "The TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF. Let’s consider an example.\n",
        "\n",
        "Sentence A = The Car is Driven on the Road <br>\n",
        "Sentence B = The Truck is Driven on the highway <br>\n",
        "\n",
        "Computation of TF-IDF scores are shown below\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*q3qYevXqQOjJf6Pwdlx8Mw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decfeeec",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:22.349367Z",
          "iopub.status.busy": "2021-12-07T14:22:22.348641Z",
          "iopub.status.idle": "2021-12-07T14:22:22.362144Z",
          "shell.execute_reply": "2021-12-07T14:22:22.362714Z",
          "shell.execute_reply.started": "2021-12-07T13:17:25.340031Z"
        },
        "papermill": {
          "duration": 0.092602,
          "end_time": "2021-12-07T14:22:22.362886",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.270284",
          "status": "completed"
        },
        "tags": [],
        "id": "decfeeec",
        "outputId": "0835aef5-4e06-41c6-ab85-257a9f4023d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF Values for sample corpus : [1.         1.28768207 1.69314718 1.28768207 1.         1.69314718]\n",
            "TF-IDF Representation for sentence 'the cat sat in the hat' :\n",
            "[[0.29903422 0.385061   0.50630894 0.385061   0.59806843 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "tfidf_rep = tfidf.fit_transform(sample_corpus)\n",
        "print(f\"IDF Values for sample corpus : {tfidf.idf_}\")\n",
        "\n",
        "\n",
        "print(\"TF-IDF Representation for sentence 'the cat sat in the hat' :\") \n",
        "print(tfidf.transform([\"the cat sat in the hat\"]).toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58840a00",
      "metadata": {
        "papermill": {
          "duration": 0.076534,
          "end_time": "2021-12-07T14:22:22.515592",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.439058",
          "status": "completed"
        },
        "tags": [],
        "id": "58840a00"
      },
      "source": [
        "Similar to BoW, we can use the TF-IDF vectors to calculate the similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information\n",
        "retrieval and text classification. However, even though TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, **it still suffers from the curse of high dimensionality.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "VWYCVKvfUH4u"
      },
      "id": "VWYCVKvfUH4u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['target']"
      ],
      "metadata": {
        "id": "8wd3q1mYUJd8"
      },
      "id": "8wd3q1mYUJd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "SvkKOm_rUMZ8"
      },
      "id": "SvkKOm_rUMZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "\n",
        "print('Accuracy of logistic regression classifier:', accuracy_score(y_test, y_pred_logreg))\n",
        "print('Confusion matrix of logistic regression classifier:')\n",
        "print(confusion_matrix(y_test, y_pred_logreg))"
      ],
      "metadata": {
        "id": "WRVOy0PIUN0U",
        "outputId": "00ab191e-ef5f-41c2-8267-04fb119e66ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WRVOy0PIUN0U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of logistic regression classifier: 0.8037340619307832\n",
            "Confusion matrix of logistic regression classifier:\n",
            "[[ 464  388   32]\n",
            " [ 115 2659   40]\n",
            " [  96  191  407]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(80,),early_stopping=True, max_iter=50)\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test)\n",
        "\n",
        "print('Accuracy of MLP classifier:', accuracy_score(y_test, y_pred_mlp))\n",
        "print('Confusion matrix of MLP classifier:')\n",
        "print(confusion_matrix(y_test, y_pred_mlp))"
      ],
      "metadata": {
        "id": "mVnotVceUPqk",
        "outputId": "3d94e2b8-f829-47c9-e2c5-fc41f0b40cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mVnotVceUPqk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of MLP classifier: 0.8037340619307832\n",
            "Confusion matrix of MLP classifier:\n",
            "[[ 464  382   38]\n",
            " [ 115 2667   32]\n",
            " [  87  208  399]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c38a382",
      "metadata": {
        "papermill": {
          "duration": 0.076053,
          "end_time": "2021-12-07T14:22:22.668690",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.592637",
          "status": "completed"
        },
        "tags": [],
        "id": "6c38a382"
      },
      "source": [
        "**Here are the main advantages and disadvantages of TF-IDF Representation:**\n",
        "\n",
        "1. Its Implementation is not that easy as compared to techniques discussed above \n",
        "2. We have a fixed-length encoding for any sentence of arbitrary length.\n",
        "3. The feature vectors are high-dimensional representations. The dimensionality increases with the size of the vocabulary.\n",
        "4. It did capture a bit of the semantics of the sentence. \n",
        "5. They too cannot handle OOV words.\n",
        "\n",
        "With this, we come to the end of basic vectorization approaches. Now, let’s start looking at distributed representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff66412",
      "metadata": {
        "papermill": {
          "duration": 0.076367,
          "end_time": "2021-12-07T14:22:22.821093",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.744726",
          "status": "completed"
        },
        "tags": [],
        "id": "dff66412"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Word2vec Word Embeddings</h1><a id = \"8\" ></a>\n",
        "\n",
        "**Word Embeddings** : They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector \n",
        "\n",
        "Word2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n",
        "\n",
        "It is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.\n",
        "\n",
        "**Using Pre-trained word2vec word embeddings** <br>\n",
        "Training your own word embeddings is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it’s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings\n",
        "can be downloaded and used to get the vectors for the words you want.  \n",
        "\n",
        "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.\n",
        "\n",
        "Below code, cell demonstrates how to use pre-trained word2vec word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "api.info()"
      ],
      "metadata": {
        "id": "CM9JIvA-fg_p",
        "outputId": "271ce570-b10e-453f-ac83-9e0c3f957d2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CM9JIvA-fg_p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'semeval-2016-2017-task3-subtaskBC': {'num_records': -1,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6344358,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'fields': {'2016-train': ['...'],\n",
              "    '2016-dev': ['...'],\n",
              "    '2017-test': ['...'],\n",
              "    '2016-test': ['...']},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'parts': 1},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'num_records': 189941,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 234373151,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'fields': {'THREAD_SEQUENCE': '',\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'RelComments': [{'RelCText': 'text of answer',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RELC_DATE': 'date of posting'}]},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'parts': 1},\n",
              "  'patent-2017': {'num_records': 353197,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 3087262469,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'parts': 2},\n",
              "  'quora-duplicate-questions': {'num_records': 404290,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 21684784,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'fields': {'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise'},\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'parts': 1},\n",
              "  'wiki-english-20171001': {'num_records': 4924894,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6516051717,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'parts': 4},\n",
              "  'text8': {'num_records': 1701,\n",
              "   'record_format': 'list of str (tokens)',\n",
              "   'file_size': 33182058,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'file_name': 'text8.gz',\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'parts': 1},\n",
              "  'fake-news': {'num_records': 12999,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 20102776,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'fields': {'crawled': 'date the story was archived',\n",
              "    'ord_in_thread': '',\n",
              "    'published': 'date published',\n",
              "    'participants_count': 'number of participants',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'replies_count': 'number of replies',\n",
              "    'main_img_url': 'image from story',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'uuid': 'unique identifier',\n",
              "    'language': 'data from webhose.io',\n",
              "    'title': 'title of story',\n",
              "    'country': 'data from webhose.io',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'likes': 'number of Facebook likes'},\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'parts': 1},\n",
              "  '20-newsgroups': {'num_records': 18846,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 14483581,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'fields': {'topic': 'name of topic (20 variant of possible values)',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'data': '',\n",
              "    'id': 'original id inferred from folder name'},\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'parts': 1},\n",
              "  '__testing_matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 1},\n",
              "  '__testing_multipart-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 3}},\n",
              " 'models': {'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
              "   'file_size': 1005007116,\n",
              "   'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'parts': 1},\n",
              "  'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
              "   'file_size': 1225497562,\n",
              "   'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-ruscorpora-300': {'num_records': 184973,\n",
              "   'file_size': 208427381,\n",
              "   'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-google-news-300': {'num_records': 3000000,\n",
              "   'file_size': 1743563840,\n",
              "   'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-50': {'num_records': 400000,\n",
              "   'file_size': 69182535,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-100': {'num_records': 400000,\n",
              "   'file_size': 134300434,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-200': {'num_records': 400000,\n",
              "   'file_size': 264336934,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-300': {'num_records': 400000,\n",
              "   'file_size': 394362229,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-25': {'num_records': 1193514,\n",
              "   'file_size': 109885004,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 25},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-50': {'num_records': 1193514,\n",
              "   'file_size': 209216938,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-100': {'num_records': 1193514,\n",
              "   'file_size': 405932991,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-200': {'num_records': 1193514,\n",
              "   'file_size': 795373100,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'parts': 1},\n",
              "  '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': [],\n",
              "   'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parts': 1}}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "Word2VecModel = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "-olCDTLuavXr",
        "outputId": "9e8a18b8-b2e7-4dfd-d18c-1e3290059de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-olCDTLuavXr",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.9% 1661.2/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb37176",
      "metadata": {
        "papermill": {
          "duration": 0.07589,
          "end_time": "2021-12-07T14:23:39.298193",
          "exception": false,
          "start_time": "2021-12-07T14:23:39.222303",
          "status": "completed"
        },
        "tags": [],
        "id": "8bb37176"
      },
      "source": [
        "As we learned Word2vec do have a similar vector representation for words with the same meaning so let's check the similar words for the word \"good\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687a5264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:23:39.456724Z",
          "iopub.status.busy": "2021-12-07T14:23:39.456027Z",
          "iopub.status.idle": "2021-12-07T14:23:42.585042Z",
          "shell.execute_reply": "2021-12-07T14:23:42.586729Z",
          "shell.execute_reply.started": "2021-12-07T13:18:39.625000Z"
        },
        "papermill": {
          "duration": 3.212874,
          "end_time": "2021-12-07T14:23:42.587205",
          "exception": false,
          "start_time": "2021-12-07T14:23:39.374331",
          "status": "completed"
        },
        "tags": [],
        "id": "687a5264",
        "outputId": "166b0e7c-b6ad-4af4-b157-ec556d29775c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348937988281), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728254318237), ('solid', 0.5806034803390503), ('lousy', 0.5764201879501343)]\n"
          ]
        }
      ],
      "source": [
        "print(Word2VecModel.most_similar('good'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5debc1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:23:42.813344Z",
          "iopub.status.busy": "2021-12-07T14:23:42.812394Z",
          "iopub.status.idle": "2021-12-07T14:23:42.816473Z",
          "shell.execute_reply": "2021-12-07T14:23:42.817001Z",
          "shell.execute_reply.started": "2021-12-07T13:18:42.030295Z"
        },
        "papermill": {
          "duration": 0.142583,
          "end_time": "2021-12-07T14:23:42.817179",
          "exception": false,
          "start_time": "2021-12-07T14:23:42.674596",
          "status": "completed"
        },
        "tags": [],
        "id": "6c5debc1",
        "outputId": "4ca6c7b0-8bdc-4770-9c12-318f8823f5a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.04052734  0.0625     -0.01745605  0.07861328  0.03271484 -0.01263428\n",
            "  0.00964355  0.12353516 -0.02148438  0.15234375 -0.05834961 -0.10644531\n",
            "  0.02124023  0.13574219 -0.13183594  0.17675781  0.27148438  0.13769531\n",
            " -0.17382812 -0.14160156 -0.03076172  0.19628906 -0.03295898  0.125\n",
            "  0.25390625  0.12695312 -0.15234375  0.03198242  0.01135254 -0.01361084\n",
            " -0.12890625  0.01019287  0.23925781 -0.08447266  0.140625    0.13085938\n",
            " -0.04516602  0.06494141  0.02539062  0.05615234  0.24609375 -0.20507812\n",
            "  0.23632812 -0.00860596 -0.02294922  0.05078125  0.10644531 -0.03564453\n",
            "  0.08740234 -0.05712891  0.08496094  0.23535156 -0.10107422 -0.03564453\n",
            " -0.04736328  0.04736328 -0.14550781 -0.10986328  0.14746094 -0.23242188\n",
            " -0.07275391  0.19628906 -0.37890625 -0.07226562  0.04833984  0.11914062\n",
            "  0.06103516 -0.12109375 -0.27929688  0.05200195  0.04907227 -0.02709961\n",
            "  0.1328125   0.03369141 -0.32226562  0.04223633 -0.08789062  0.15429688\n",
            "  0.09472656  0.10351562 -0.02856445  0.00128174 -0.00427246  0.24609375\n",
            " -0.05957031 -0.16894531 -0.09619141  0.16796875  0.0133667   0.04882812\n",
            "  0.08349609  0.06347656 -0.00872803 -0.08642578 -0.03857422 -0.08251953\n",
            "  0.15722656  0.22753906 -0.00762939 -0.19921875 -0.06347656  0.12792969\n",
            " -0.06347656 -0.03027344  0.0456543   0.06298828 -0.02526855 -0.06787109\n",
            " -0.01141357 -0.13574219  0.02978516  0.10400391 -0.15917969 -0.08447266\n",
            "  0.29882812 -0.12597656  0.11425781 -0.08105469 -0.09082031 -0.07910156\n",
            " -0.11181641 -0.09619141  0.02770996  0.14257812 -0.26757812 -0.09375\n",
            "  0.03979492 -0.17871094 -0.02819824  0.01464844 -0.31640625 -0.24511719\n",
            " -0.08935547  0.09716797 -0.00964355 -0.14746094  0.15234375  0.21582031\n",
            "  0.05981445  0.23828125 -0.05151367  0.14941406  0.13574219 -0.03222656\n",
            " -0.265625   -0.11181641 -0.23046875 -0.140625    0.25585938 -0.15429688\n",
            "  0.1796875   0.15527344 -0.21582031  0.36328125 -0.1015625   0.04980469\n",
            "  0.07177734 -0.14550781 -0.03198242  0.00952148 -0.12109375  0.12109375\n",
            "  0.09765625  0.07763672  0.3203125  -0.22265625 -0.08447266 -0.10742188\n",
            "  0.11279297 -0.13867188 -0.21875     0.0145874   0.13378906 -0.00921631\n",
            "  0.00921631  0.16894531  0.16894531 -0.078125   -0.00665283  0.03735352\n",
            " -0.10888672 -0.25390625  0.01452637 -0.09716797 -0.19628906 -0.01782227\n",
            " -0.28125    -0.02050781 -0.02905273 -0.09375    -0.17675781  0.21484375\n",
            " -0.05224609 -0.11572266 -0.01977539 -0.10839844 -0.01342773 -0.15332031\n",
            " -0.140625   -0.11816406  0.09228516  0.109375    0.05761719 -0.03466797\n",
            "  0.03564453 -0.12011719 -0.14257812 -0.00072479 -0.06689453  0.11914062\n",
            " -0.10449219  0.07861328 -0.12792969  0.09570312 -0.00817871  0.07128906\n",
            "  0.20703125 -0.03149414  0.09570312  0.17285156 -0.07958984 -0.02429199\n",
            " -0.07519531 -0.07568359  0.09521484 -0.06494141 -0.00689697 -0.09033203\n",
            "  0.03100586  0.19921875 -0.10644531 -0.11474609  0.18652344 -0.05078125\n",
            "  0.0859375   0.00128937 -0.18847656 -0.20019531 -0.02832031  0.11328125\n",
            "  0.25976562  0.22070312  0.04101562  0.00171661  0.07568359 -0.01196289\n",
            "  0.0177002  -0.05883789 -0.25976562 -0.234375   -0.04956055  0.25976562\n",
            "  0.15332031  0.15136719  0.08300781 -0.15527344  0.04931641  0.07519531\n",
            " -0.05078125 -0.1328125  -0.13574219  0.04199219 -0.14257812  0.02099609\n",
            "  0.07861328  0.01611328  0.01623535 -0.21582031  0.01599121 -0.04882812\n",
            " -0.02404785  0.13476562  0.08496094 -0.01196289  0.10009766 -0.13867188\n",
            "  0.08056641 -0.22070312 -0.12011719  0.18945312  0.05444336 -0.05053711\n",
            "  0.00147247  0.14160156 -0.06494141 -0.05566406 -0.09033203 -0.0267334\n",
            " -0.10498047  0.02416992  0.01422119  0.1875     -0.16503906  0.01538086\n",
            " -0.04174805  0.05444336 -0.01184082 -0.15625     0.00193024 -0.06982422]\n"
          ]
        }
      ],
      "source": [
        "print(Word2VecModel['good'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf2887b",
      "metadata": {
        "papermill": {
          "duration": 0.07775,
          "end_time": "2021-12-07T14:23:42.972094",
          "exception": false,
          "start_time": "2021-12-07T14:23:42.894344",
          "status": "completed"
        },
        "tags": [],
        "id": "cdf2887b"
      },
      "source": [
        "In the output of the above code cell, we can see a 300-dimensional real-valued vector for the word \"good\".\n",
        "\n",
        "The above few cells were about using pre-trained word2vec representation now in upcoming cells we will focus on learning/Training our own word2vec representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4958bcef",
      "metadata": {
        "papermill": {
          "duration": 0.076746,
          "end_time": "2021-12-07T14:23:43.126254",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.049508",
          "status": "completed"
        },
        "tags": [],
        "id": "4958bcef"
      },
      "source": [
        "**Training our own embeddings**\n",
        "\n",
        "Now we’ll focus on training our own word embeddings. For this, we’ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:\n",
        "\n",
        "1. Continuous bag of words (CBOW)\n",
        "2. SkipGram\n",
        "\n",
        "Both of these have a lot of similarities in many respects. \n",
        "\n",
        "Throughout this section, we’ll use the sentence “The quick brown fox jumps over the lazy dog” as our example text.\n",
        "\n",
        "**1. Continuous bag of words (CBOW)**\n",
        "\n",
        "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word.\n",
        "<br><br>\n",
        "Now next task is to create a training sample of the form (X, Y) for this task where X will be context words and Y will be Center word. We define the value of context window = 2 in this case.<br> \n",
        "![](https://miro.medium.com/max/536/1*vZhxrBkCz-yN_rzZBqSKiA.png)\n",
        "\n",
        "<br><br>\n",
        "Now that we have the training data ready, let’s focus on the model. For this, we construct a shallow net (it’s shallow since it has a single hidden layer). We assume we want to learn D-dim word embeddings. Further, let V be the vocabulary of the text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa4b64d",
      "metadata": {
        "papermill": {
          "duration": 0.078419,
          "end_time": "2021-12-07T14:23:43.280598",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.202179",
          "status": "completed"
        },
        "tags": [],
        "id": "dfa4b64d"
      },
      "source": [
        "<img src=\"https://file.techscience.com/ueditor/files/TSP_CSSE_36-1/TSP_CSSE_14260/TSP_CSSE_14260/images/fig-1.png\" width=\"600\">\n",
        "\n",
        "For more information you can checkout https://lilianweng.github.io/posts/2017-10-15-word-embedding/\n",
        "<br><br>\n",
        "The objective is to learn an embedding matrix W|V| x d.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of the embedding. Let’s break down the shallow net in Figure layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix W|V| x d. The vectors fetched are then added to get a single D-dim vector, and this is passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix W’d x |V|.. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn.\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c5ab1b",
      "metadata": {
        "papermill": {
          "duration": 0.086327,
          "end_time": "2021-12-07T14:23:43.464620",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.378293",
          "status": "completed"
        },
        "tags": [],
        "id": "41c5ab1b"
      },
      "source": [
        "**2. SkipGram** \n",
        "\n",
        "SkipGram is very similar to CBOW, with some minor changes. In Skip‐ Gram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word “jumps,” we try to predict every word in context—“brown,” “fox,” “over,” “the”.\n",
        "\n",
        "Now we will create a training sample of the form (X, Y) for this task where X will be the center word and Y will be Context words. \n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*jkxbwD55_8M3XBRb1bGm7A.png\" width=\"600\">\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://pub.mdpi-res.com/futureinternet/futureinternet-11-00114/article_deploy/html/images/futureinternet-11-00114-g001.png?1571521360\" width=\"600\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "The shallow network used to train the SkipGram model, shown in the below Figure, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix W|V| x d. The vectors fetched are then passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix W’d x |V|. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices W and W’ accordingly. At the end of the training, W is the embedding matrix we wanted to learn.\n",
        "\n",
        "For more information you can checkout https://lilianweng.github.io/posts/2017-10-15-word-embedding/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0696ba4c",
      "metadata": {
        "papermill": {
          "duration": 0.08588,
          "end_time": "2021-12-07T14:23:43.634447",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.548567",
          "status": "completed"
        },
        "tags": [],
        "id": "0696ba4c"
      },
      "source": [
        "**Implementation** : \n",
        "\n",
        "One of the most commonly used implementations is with gensim. We have to choose several hyperparameters (i.e., the variables that need to be set before starting the training process). Let’s look at two examples.\n",
        "\n",
        "Dimensionality of the word vectors\n",
        "\n",
        "As the name indicates, this decides the space of the learned embeddings. While there is no ideal number, it’s common to construct word vectors with dimensions in the range of 50–500 and evaluate them on the task we’re using them for to choose the best option. In gensim we do this by setting the \"vector_size\" parameter to the size we want. \n",
        "\n",
        "Context window\n",
        "\n",
        "How long or short the context we look for to learn the vector representation is. In gensim we do this by setting the \"window\" parameter to the size we want.\n",
        "\n",
        "There are also other choices we make, such as whether to use CBOW or SkipGram to\n",
        "learn the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf09d92",
      "metadata": {
        "papermill": {
          "duration": 0.077271,
          "end_time": "2021-12-07T14:23:44.849036",
          "exception": false,
          "start_time": "2021-12-07T14:23:44.771765",
          "status": "completed"
        },
        "tags": [],
        "id": "ddf09d92"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Glove Word Embeddings</h1><a id = \"9\" ></a>\n",
        "\n",
        "GloVe Stands for Global Vectors for word representation is another word embedding technique that was developed as an open-source project at Stanford and was launched in 2014. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. But keep in mind that there’s quite a bit of synergy between the GloVe and Word2vec. The gloVe can be used to find relations between words like synonyms, company-product relations, zip codes, and cities, etc.\n",
        "\n",
        "The question may arise Why do we need Glove if we have word2vec as a good word embedding technique Because Word2vec relies only on local information of language. That is, the semantics learned for a given word, are only affected by the surrounding words.\n",
        "\n",
        "For example, take the sentence,\n",
        "\n",
        "The cat sat on the mat\n",
        "\n",
        "If you use Word2vec, it wouldn’t capture information like,\n",
        "\n",
        "is “the” a special context of the words “cat” and “mat” ?\n",
        "\n",
        "or\n",
        "\n",
        "is “the” just a stopword?\n",
        "\n",
        "This can be suboptimal, especially in the eye of theoreticians.\n",
        "\n",
        "GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n",
        "\n",
        "![](https://miro.medium.com/max/434/1*QWcK8CIDs8kMkOwsOxvywA.png)\n",
        "\n",
        "The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix.\n",
        "\n",
        "For detailed knowledge about Glove word embedding, you can refer [This article](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb57ec6c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:23:45.011448Z",
          "iopub.status.busy": "2021-12-07T14:23:45.009576Z",
          "iopub.status.idle": "2021-12-07T14:25:34.924552Z",
          "shell.execute_reply": "2021-12-07T14:25:34.925132Z",
          "shell.execute_reply.started": "2021-12-07T13:18:42.327261Z"
        },
        "papermill": {
          "duration": 109.998348,
          "end_time": "2021-12-07T14:25:34.925334",
          "exception": false,
          "start_time": "2021-12-07T14:23:44.926986",
          "status": "completed"
        },
        "tags": [],
        "id": "cb57ec6c",
        "outputId": "14b16dce-8c25-4506-ec7f-5dfb936cbb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "Glove_model = api.load(\"glove-twitter-25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a95065",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:26:49.772334Z",
          "iopub.status.busy": "2021-12-07T14:26:49.771637Z",
          "iopub.status.idle": "2021-12-07T14:26:49.912017Z",
          "shell.execute_reply": "2021-12-07T14:26:49.912892Z",
          "shell.execute_reply.started": "2021-12-07T13:21:45.496829Z"
        },
        "papermill": {
          "duration": 0.224579,
          "end_time": "2021-12-07T14:26:49.913184",
          "exception": false,
          "start_time": "2021-12-07T14:26:49.688605",
          "status": "completed"
        },
        "tags": [],
        "id": "38a95065",
        "outputId": "742d02a1-559b-4d5a-acc0-f372addddcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to word 'human' : \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('common', 0.9221445918083191),\n",
              " ('powerful', 0.9064364433288574),\n",
              " ('example', 0.9053514003753662),\n",
              " ('society', 0.8996164202690125),\n",
              " ('age', 0.8994824290275574),\n",
              " ('freedom', 0.8900521993637085),\n",
              " ('child', 0.88453209400177),\n",
              " ('which', 0.8807503581047058),\n",
              " ('knowledge', 0.8800180554389954),\n",
              " ('woman', 0.8785547018051147)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "print(\"Most similar words to word 'human' : \")\n",
        "Glove_model.most_similar('human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85d69b2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:26:50.127328Z",
          "iopub.status.busy": "2021-12-07T14:26:50.126361Z",
          "iopub.status.idle": "2021-12-07T14:26:50.131030Z",
          "shell.execute_reply": "2021-12-07T14:26:50.131512Z",
          "shell.execute_reply.started": "2021-12-07T13:21:45.615577Z"
        },
        "papermill": {
          "duration": 0.093864,
          "end_time": "2021-12-07T14:26:50.131686",
          "exception": false,
          "start_time": "2021-12-07T14:26:50.037822",
          "status": "completed"
        },
        "tags": [],
        "id": "e85d69b2",
        "outputId": "9ce907b9-dad9-4afe-b6b4-bcb5c9e7c0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glove Word Embeddings of word 'human' \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.23616, -0.21154, -1.0123 ,  0.57591,  0.99242, -0.19829,\n",
              "        1.5235 , -0.43865,  0.31782,  0.47775,  0.26363,  0.26403,\n",
              "       -3.7663 ,  0.53193, -0.28841,  0.28303,  0.3007 ,  0.56851,\n",
              "        1.0938 , -0.31324, -0.16352,  0.48272, -0.90641, -0.69115,\n",
              "       -0.46283], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "print(\"Glove Word Embeddings of word 'human' \")\n",
        "Glove_model['human']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0760834a",
      "metadata": {
        "papermill": {
          "duration": 0.079887,
          "end_time": "2021-12-07T14:26:50.292131",
          "exception": false,
          "start_time": "2021-12-07T14:26:50.212244",
          "status": "completed"
        },
        "tags": [],
        "id": "0760834a"
      },
      "source": [
        "**Advantages of Word2Vec:**\n",
        "\n",
        "1. The idea is very intuitive, which transforms the unlabeled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n",
        "2. The data can be fed into the model in an online way and needs little preprocessing, thus requiring little memory.\n",
        "3. the mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like “king:man as queen: woman” can be inferred by word vectors.\n",
        "3. It is simple for a freshman to understand the principle and do the implementation.\n",
        "\n",
        "**Disadvantages of Word2Vec:**\n",
        "\n",
        "1. The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristics.\n",
        "2. The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary). Though approximation algorithms like negative sampling (NEG) and hierarchical softmax (HS) are proposed to address the issue, other problems happen. For example, the word vectors by NEG are not distributed uniformly, they are located within a cone in the vector space hence the vector space is not sufficiently utilized.\n",
        "3. It still provides no way to address the **out of vocabulary(OOV)** problem.\n",
        "\n",
        "**Advantages of Glove:**\n",
        "\n",
        "1. The goal of Glove is very straightforward, i.e., to enforce the word vectors to capture sub-linear relationships in the vector space. Thus, it proves to perform better than Word2vec in the word analogy tasks.\n",
        "2. Glove adds some more practical meaning to word vectors by considering the relationships between word pair and word pair rather than word and word.\n",
        "3. Glove gives lower weight for highly frequent word pairs to prevent the meaningless stop words like “the”, “an” will not dominating the training progress.\n",
        "\n",
        "**Disadvantages of Glove:**\n",
        "\n",
        "1. The model is trained on the co-occurrence matrix of words, which takes a lot of memory for storage. Especially, if you change the hyper-parameters related to the co-occurrence matrix, you have to reconstruct the matrix again, which is very time-consuming.\n",
        "\n",
        "**Both Word2vec and Glove do not solve the problems like:**\n",
        "\n",
        "1. How to learn the representation for out-of-vocabulary words.\n",
        "2. How to separate some opposite word pairs. For example, “good” and “bad” are usually located very close to each other in the vector space, which may limit the performance of word vectors in NLP tasks like sentiment analysis.\n",
        "\n",
        "Content of this cell is taken from [Quora](https://www.quora.com/What-are-the-advantages-and-disadvantages-of-Word2vec-and-GloVe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cf12a0",
      "metadata": {
        "papermill": {
          "duration": 0.080012,
          "end_time": "2021-12-07T14:26:50.451727",
          "exception": false,
          "start_time": "2021-12-07T14:26:50.371715",
          "status": "completed"
        },
        "tags": [],
        "id": "a6cf12a0"
      },
      "source": [
        "Despite the ease of using Powerfull Word Embeddings like Word2vec or any such word embedding to do feature extraction from texts, **we don’t have a good way of handling OOV words yet**.\n",
        "\n",
        "What We can do to solve this Problem?\n",
        "\n",
        "1. A simple approach that often works is to exclude those words from the feature extraction process so we don’t have to worry about how to get their representations.\n",
        "\n",
        "2. Another way to deal with the OOV problem for word embeddings is to create vectors that are initialized randomly.\n",
        "\n",
        "3. There are also other approaches that handle the OOV problem by modifying the training process by bringing in characters and other subword-level linguistic components. Let’s look at one such approach now. The key idea is that one can potentially handle the OOV problem by using subword information, such as morphological properties (e.g., prefixes, suffixes, word endings, etc.), **or by using character representations. fastText, from Facebook AI research**, is one of the popular algorithms that follows this approach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8224cc0",
      "metadata": {
        "papermill": {
          "duration": 0.079608,
          "end_time": "2021-12-07T14:26:50.611344",
          "exception": false,
          "start_time": "2021-12-07T14:26:50.531736",
          "status": "completed"
        },
        "tags": [],
        "id": "e8224cc0"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">FastText Word Embeddings</h1><a id = \"10\" ></a>\n",
        "\n",
        "A word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word’s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there’s a word, “gregarious,” that’s not found in the embedding’s word vocabulary. We break it into character n-grams—gre, reg, ega, ….ous—and combine these embeddings of the ngrams to arrive at the embedding of “gregarious.”\n",
        "\n",
        "How FastText Works?\n",
        "\n",
        "FastText is a modified version of word2vec (i.e.. Skip-Gram and CBOW). The only difference between fastText vs word2vec is its pooling strategies (what are the input, output, and dictionary of the model). In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.\n",
        "\n",
        "**character n-grams** the contiguous sequence of n items from a given sample of a character or word. It may be bigram, trigram, etc.\n",
        "For example character trigram (n = 3) of the word “where” will be:\n",
        "\n",
        "<wh, whe, her, ere, re>\n",
        "\n",
        "In FastText architecture, they have also included the word itself with the character n-gram. That means input data to the model for the word “eating” will be:\n",
        "\n",
        "![](https://amitness.com/images/fasttext-center-word-embedding.png)\n",
        "\n",
        "Now the model I am referring same is word2vec which is a shallow neural network with one hidden layer as discussed above.\n",
        "\n",
        "\n",
        "Now to prepare training data for the (Skip-Gram-based) FastText model, we define “context word” as the word which follows a given word in the text (which will be our “target word”). That means we will be predicting the surrounding word for a given word.\n",
        "\n",
        "Note: FastText word embeddings support both Continuous Bag of Words (CBOW) and Skip-Gram models. I will explain and implement the skip-gram model in the below cell to learn vector representation (FastText word embeddings). Now let’s construct our training examples (like Skip-Gram), scanning through the text with a window will prepare a context word and a target word.\n",
        "\n",
        "Consider the sentence : \n",
        "\n",
        "<div style = \"text-align:center\"><b> i like natural language processing</b></div>\n",
        "\n",
        "![](https://secureservercdn.net/45.40.148.234/um0.ec8.myftpupload.com/wp-content/uploads/2020/10/Picture2.png)\n",
        "\n",
        "For the above example, for context words “i” and “natural” the target word will be “like”. Full training data for FastText word embedding will look like below. By observing the below training data, your confusion of fastText vs word2vec should be clear.\n",
        "\n",
        "\n",
        "Now you know in word2vec (skip-gram) each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram. This training data preparation is the only difference between FastText word embeddings and skip-gram (or CBOW) word embeddings.\n",
        "\n",
        "After training data preparation of FastText, training the word embedding, finding word similarity, etc. are the same as the word2vec model (for our example similar to the skip-gram model).\n",
        "\n",
        "Now let’s see how to implement FastText word embeddings in python using Gensim library.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
      ],
      "metadata": {
        "id": "k28Z3J14gVq6",
        "outputId": "90ae13eb-e325-4dc4-ff90-9acfb5ddd6aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "id": "k28Z3J14gVq6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d4cea9496a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fasttext-wiki-news-subwords-300\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1cfdf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:26:50.777570Z",
          "iopub.status.busy": "2021-12-07T14:26:50.776795Z",
          "iopub.status.idle": "2021-12-07T14:35:55.608023Z",
          "shell.execute_reply": "2021-12-07T14:35:55.608877Z",
          "shell.execute_reply.started": "2021-12-07T13:21:45.643617Z"
        },
        "papermill": {
          "duration": 544.917726,
          "end_time": "2021-12-07T14:35:55.609232",
          "exception": false,
          "start_time": "2021-12-07T14:26:50.691506",
          "status": "completed"
        },
        "tags": [],
        "id": "1e1cfdf0"
      },
      "outputs": [],
      "source": [
        "print(\"Most similar words to word 'human' : \")\n",
        "fasttext_model.most_similar('human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc78fd1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:35:55.933179Z",
          "iopub.status.busy": "2021-12-07T14:35:55.932080Z",
          "iopub.status.idle": "2021-12-07T14:35:55.937079Z",
          "shell.execute_reply": "2021-12-07T14:35:55.937583Z",
          "shell.execute_reply.started": "2021-12-07T13:39:57.957729Z"
        },
        "papermill": {
          "duration": 0.204765,
          "end_time": "2021-12-07T14:35:55.937752",
          "exception": false,
          "start_time": "2021-12-07T14:35:55.732987",
          "status": "completed"
        },
        "tags": [],
        "id": "1bc78fd1"
      },
      "outputs": [],
      "source": [
        "print(\"Glove Word Embeddings of word 'human' \")\n",
        "fasttext_model['human']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def get_embedding_matrix(text):\n",
        "    words = text.split()\n",
        "    embeddings = [fasttext_model[w] for w in words if w in fasttext_model]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(fasttext_model.vector_size)\n",
        "\n",
        "# Convert the training data into embedding matrices\n",
        "X_train = np.array([get_embedding_matrix(x) for x in train_df['text']])\n",
        "y_train = train_df['target']\n",
        "\n",
        "# Convert the testing data into embedding matrices\n",
        "X_test = np.array([get_embedding_matrix(x) for x in test_df['text']])\n",
        "y_test = test_df['target']"
      ],
      "metadata": {
        "id": "36lLHMFzlwkw"
      },
      "id": "36lLHMFzlwkw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a logistic regression model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print('Accuracy of logistic regression classifier:', accuracy_score(y_test, y_pred))\n",
        "print('Confusion matrix of logistic regression classifier:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "puHAIpAokMsx"
      },
      "id": "puHAIpAokMsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(40,),early_stopping=True, max_iter=20)\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test)\n",
        "\n",
        "print('Accuracy of MLP classifier:', accuracy_score(y_test, y_pred_mlp))\n",
        "print('Confusion matrix of MLP classifier:')\n",
        "print(confusion_matrix(y_test, y_pred_mlp))"
      ],
      "metadata": {
        "id": "dCYlNsDOmIHz"
      },
      "id": "dCYlNsDOmIHz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "45da4ad8",
      "metadata": {
        "papermill": {
          "duration": 0.08318,
          "end_time": "2021-12-07T14:35:58.185552",
          "exception": false,
          "start_time": "2021-12-07T14:35:58.102372",
          "status": "completed"
        },
        "tags": [],
        "id": "45da4ad8"
      },
      "source": [
        "<b>A potential problem with both approaches is that they do not take the context of words into account. Take, for example, the sentences “cat sat on mat” and “mat sat on cat.” Both receive the same representation in these approaches, but they obviously have very different meanings.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">RNNs</h1><a id = \"10\" ></a>\n",
        "Recurrent Neural Networks (RNNs) are a type of neural network designed for processing sequential data. They operate on a sequence of input data, and they maintain an internal state (i.e., hidden state) that allows them to capture information from earlier parts of the sequence and use it to help process later parts of the sequence.\n",
        "\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "\n",
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">LSTM</h1><a id = \"10\" ></a>\n",
        "Long Short-Term Memory (LSTM) networks are a type of RNN that address the vanishing gradient problem that can occur in standard RNNs. In standard RNNs, the gradients can become very small as they are backpropagated through the network, which can cause the network to have difficulty learning long-term dependencies in the input sequence. LSTMs solve this problem by introducing additional gating mechanisms that allow the network to selectively store and access information in the hidden state.\n",
        "\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">BiLSTMs</h1><a id = \"10\" ></a>\n",
        "Bidirectional LSTMs (BiLSTMs) are a type of LSTM that allow information from both past and future time steps to be incorporated into the hidden state. BiLSTMs achieve this by processing the input sequence in both forward and backward directions, and concatenating the resulting hidden states.\n",
        "\n",
        "<br><br>\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.54.27_PM.png\" width=\"500\">\n",
        "\n",
        "\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ],
      "metadata": {
        "id": "kfODK2sT_LRA"
      },
      "id": "kfODK2sT_LRA"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the input data and labels\n",
        "docs = data.text.tolist()\n",
        "labels = data.target.tolist()\n",
        "\n",
        "# Convert text data to numeric sequences\n",
        "word_to_ix = {'<PAD>': 0, 'unk': 1}\n",
        "for doc in docs:\n",
        "    for word in doc.split():\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "sequences = []\n",
        "for doc in docs:\n",
        "    seq = [word_to_ix[word] for word in doc.split()]\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Pad the sequences to have uniform length\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "X = np.zeros((len(sequences), max_len))\n",
        "for i, seq in enumerate(sequences):\n",
        "    X[i, :len(seq)] = seq\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "oKrmabAJZoUx"
      },
      "id": "oKrmabAJZoUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom PyTorch Dataset for the text data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.LongTensor(self.X[idx]), torch.LongTensor([self.y[idx]])\n",
        "\n",
        "# Create DataLoaders for the training and test sets\n",
        "batch_size = 2\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mo5ZpLOuZsn_"
      },
      "id": "mo5ZpLOuZsn_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model architecture\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True) # nn.LSTM bidirectional=True\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, hidden = self.rnn(x)\n",
        "        output = self.fc(hidden.squeeze())\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "W-H2uI3CZvz4"
      },
      "id": "W-H2uI3CZvz4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(len(word_to_ix), 32, 32, 3)\n",
        "\n",
        "# Train the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "jFtXa-lt7RJh"
      },
      "id": "jFtXa-lt7RJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', epoch_loss/len(train_loader))\n"
      ],
      "metadata": {
        "id": "i1VF9ImpZ0oP",
        "outputId": "9c50ab26-1e18-4509-936d-b26c9084b014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i1VF9ImpZ0oP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.9259759796467798\n",
            "Epoch: 2 Loss: 0.9286555921394235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "-_6PUV2QZ1uX",
        "outputId": "1e7e46b5-304c-4f00-b117-ef1b108e8881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-_6PUV2QZ1uX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6451502732240437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Task 1: Define a BiLSTM model architecture which uses pretrained word embeddings passed as parameters.</h1>"
      ],
      "metadata": {
        "id": "PfDnOO6XxiM5"
      },
      "id": "PfDnOO6XxiM5"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self,embedding_dim, hidden_dim,batch_size,num_layers, pretrained_embeddings):\n",
        "        super(BiLSTM , self).__init__()\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "        self.num_layers=num_layers\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "                          batch_first=True, bidirectional=True) ## Bidirectional RNN\n",
        "        self.linear = nn.Linear(2*hidden_dim, 3) ## Input dimension are 2 times hidden dimensions due to bidirectional results\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings, torch.randn(2*self.num_layers, len(X_batch), self.hidden_dim))\n",
        "        \n",
        "        return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "2mfDbPc3zawC"
      },
      "id": "2mfDbPc3zawC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictonary of embeddings of words in vocabulary\n",
        "pretrained_embeddings = np.zeros((len(word_to_ix), 25))\n",
        "for word, idx in word_to_ix.items():\n",
        "    if word in Glove_model.vocab:\n",
        "        pretrained_embeddings[idx] = Glove_model[word]\n",
        "    else:\n",
        "        pretrained_embeddings[idx] = Glove_model['unk']"
      ],
      "metadata": {
        "id": "g7iR3NqTzefE"
      },
      "id": "g7iR3NqTzefE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTM(25, 10, 32, 1, torch.FloatTensor(pretrained_embeddings))"
      ],
      "metadata": {
        "id": "X1SkgwTlzfCv"
      },
      "id": "X1SkgwTlzfCv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', epoch_loss/len(train_loader))"
      ],
      "metadata": {
        "id": "6Bfyed7z2mu-",
        "outputId": "43a87716-6e86-4138-ee9c-9d1ac385fa90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6Bfyed7z2mu-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.9277273580493008\n",
            "Epoch: 2 Loss: 0.9245843655821648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "4XT_6sjJ-HfI",
        "outputId": "31c04eca-6307-4474-cc06-1d21e1c2b8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4XT_6sjJ-HfI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6451502732240437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>One of the main disadvantages of LSTMs is that they still require sequential processing of the input sequence, which can be computationally expensive for long sequences. Additionally, LSTMs may have difficulty modeling very long-term dependencies or capturing complex relationships between distant parts of the sequence.\n",
        "\n",
        "Transformers are a type of neural network architecture that were introduced in 2017. Unlike LSTMs, Transformers use a self-attention mechanism to directly model the relationships between all elements in the input sequence, rather than relying on sequential processing. This allows Transformers to efficiently capture long-range dependencies and complex relationships between elements in the sequence, without the computational overhead of processing the sequence sequentially.</b>"
      ],
      "metadata": {
        "id": "-F2cQibkCDq3"
      },
      "id": "-F2cQibkCDq3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Task 2: Visualize some of the classification examples using LIME. You will find the relevent codes in following links. Adapt the code to meet your needs.</h1>\n",
        "\n",
        "[pytorch rnn for text classification tasks](https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks)\n",
        "\n",
        "[lime repo](https://github.com/marcotcr/lime)"
      ],
      "metadata": {
        "id": "eHcqp3Z57M0i"
      },
      "id": "eHcqp3Z57M0i"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from lime import lime_text\n",
        "import numpy as np\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def make_predictions():\n",
        "  y_pred = []\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
        "   \n",
        "    return preds.detach().numpy()\n",
        "target_classes = [\"neutral\", \"negative\", \"positive\"]\n",
        "\n",
        "max_words = 25\n",
        "explainer = lime_text.LimeTextExplainer(class_names=target_classes, verbose=True)\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "idx = rng.randint(1, len(X_test))\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(torch.argmax(outputs, dim=1).tolist())\n",
        "        preds = model(torch.tensor(inputs, dtype=torch.int32))\n",
        "        #preds = F.softmax(preds, dim=-1)\n",
        "        print(preds)\n",
        "        print(\"Prediction : \", target_classes[preds[0].argmax()])\n",
        "        print(\"Actual :     \", target_classes[labels[0]])\n",
        "\n",
        "        explanation = explainer.explain_instance(inputs, classifier_fn=make_predictions,\n",
        "                                         labels=y_test[idx:idx+1])\n",
        "explanation.show_in_notebook()"
      ],
      "metadata": {
        "id": "8iFzCLTi8PYK",
        "outputId": "af28a392-f436-4562-c310-6841837046a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "id": "8iFzCLTi8PYK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1926,  1.3383, -0.0447],\n",
            "        [ 0.0225,  1.2704, -0.1629]])\n",
            "Prediction :  negative\n",
            "Actual :      positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-2b272c23c3b7>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  preds = model(torch.tensor(inputs, dtype=torch.int32))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-2b272c23c3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual :     \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         explanation = explainer.explain_instance(inputs, classifier_fn=make_predictions,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                          labels=y_test[idx:idx+1])\n\u001b[1;32m     33\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    407\u001b[0m             text_instance, bow=self.bow, mask_string=self.mask_string)\n\u001b[1;32m    408\u001b[0m                           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_level\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                           IndexedString(text_instance, bow=self.bow,\n\u001b[0m\u001b[1;32m    410\u001b[0m                                         \u001b[0msplit_expression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_expression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                                         mask_string=self.mask_string))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw_string, split_expression, bow, mask_string)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# the separator character from the split results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(%s)|$'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msplit_expression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mnon_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCoYb-mjU8qJ"
      },
      "id": "rCoYb-mjU8qJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 853.142536,
      "end_time": "2021-12-07T14:36:10.246332",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-12-07T14:21:57.103796",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}